# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Tests RNN cells."""

import jax
import jax.numpy as jnp
import numpy as np

from renn.rnn import cells


class TestLinearRNN:
  """Tests the Linear RNN dataclass."""

  def setup_method(self):

    # Define weights of the RNN cell.
    self.num_units = 5
    self.num_inputs = 1

    self.rec_weights = jnp.eye(self.num_units)
    self.inp_weights = jnp.zeros((self.num_units, self.num_inputs))
    self.bias = jnp.zeros(self.num_units,)

    # Build a Linear RNN cell using these weights.
    self.rnn = cells.LinearRNN(self.inp_weights, self.rec_weights, self.bias)

  def test_fields(self):
    """Tests fields of the Linear RNN cell."""
    assert self.rnn.A is self.inp_weights
    assert self.rnn.W is self.rec_weights
    assert self.rnn.b is self.bias

  def test_apply(self):
    """Tests the apply() method."""
    inputs = jnp.array([1.])
    state = jnp.arange(self.num_units).astype(jnp.float32)
    new_state = self.rnn.apply(inputs, state)

    # For this RNN, the new state should equal the previous state.
    assert np.allclose(state, new_state)


def test_embedding():
  """Tests the embedding (discrete lookup) layer."""

  # Setup.
  key = jax.random.PRNGKey(0)
  vocab_size = 100
  emb_size = 32
  initializer = jax.nn.initializers.orthogonal()

  # Build embedding layer.
  init_fun, apply_fun = cells.embedding(vocab_size, emb_size, initializer)

  # Initialize embedding.
  input_shape = (-1,)
  output_shape, emb = init_fun(key, input_shape)

  # Embeddings should be orthogonal.
  assert np.allclose(emb.T @ emb, jnp.eye(emb_size), atol=1e-5)

  # Output shape is the input shape with an additional embedding dimension.
  assert output_shape == input_shape + (emb_size,)

  # Apply the embedding to each possible token.
  tokens = jnp.arange(vocab_size)
  outputs = apply_fun(emb, tokens)
  assert np.allclose(outputs, emb)


class TestVanillaRNN:
  """Tests properties and methods of the Vanilla RNN class."""

  def setup_method(self):
    base_key = jax.random.PRNGKey(0)
    keys = jax.random.split(base_key, 3)

    self.num_units = 32
    self.num_inputs = 16
    self.cell = cells.VanillaRNN(self.num_units)

    input_shape = (-1, self.num_inputs)
    self.output_shape, self.params = self.cell.init(keys[0], input_shape)

    # Generate random RNN state and inputs.
    self.state = jax.random.normal(keys[1], (self.num_units,))
    self.inputs = jax.random.normal(keys[2], (self.num_inputs,))

  def test_output_shape(self):
    """Tests the output shape from the init method."""
    expected_output_shape = (-1, self.num_units)
    assert self.output_shape == expected_output_shape

  def test_params_shape(self):
    """Tests the shape of parameters generated by the init method."""
    expected_shape = {
        'initial_state': (self.num_units,),
        'weights': (
            (self.num_units, self.num_inputs),  # Input weight matrix.
            (self.num_units, self.num_units),   # Recurrent weight matrix.
            (self.num_units,))                  # Bias shape.
    }

    # Test shape of the initial state vector.
    assert self.params['initial_state'].shape == expected_shape['initial_state']

    # Test shape of the RNN cell weights.
    for weight, expected_weight_shape in zip(self.params['weights'].flatten(),
                                             expected_shape['weights']):
      assert weight.shape == expected_weight_shape

  def test_apply(self):
    """Tests the single step apply method."""
    new_state = self.cell.apply(self.params, self.inputs, self.state)

    # State shape should not change.
    assert new_state.shape == self.state.shape

    # Outputs should be bounded by (-1, +1), due to the tanh nonlinearity.
    assert np.all(new_state <= 1.)
    assert np.all(new_state >= -1.)

  def test_batch_initial_shape(self):
    """Tests method to get a batch of initial states."""
    bz = 256

    # Generate a batch of initial states.
    states = self.cell.get_initial_state(self.params, batch_size=(bz,))

    # Test shape.
    assert states.shape == (bz, self.num_units)

    # Initial states across the batch should be the same.
    assert np.allclose(np.diff(states, axis=0), 0.)

  def test_input_jacobian(self):
    """Tests the ability to compute Jacobians wrt. the inputs."""
    jac = self.cell.inp_jac(self.params, self.inputs, self.state)
    assert jac.shape == (self.num_units, self.num_inputs)

  def test_recurrent_jacobian(self):
    """Tests the ability to compute Jacobians wrt. the recurrent state."""
    jac = self.cell.rec_jac(self.params, self.inputs, self.state)
    assert jac.shape == (self.num_units, self.num_units)

  def test_batch_apply(self):
    """Tests the ability to apply the RNN to a batch of inputs."""
    batch_size = 256

    # Generate batch inputs and states.
    batch_inputs = jnp.repeat(self.inputs[jnp.newaxis, :], batch_size, axis=0)
    batch_states = jnp.repeat(self.state[jnp.newaxis, :], batch_size, axis=0)

    # Apply the RNN.
    new_states = self.cell.batch_apply(self.params, batch_inputs, batch_states)

    # Test shape.
    assert new_states.shape == (batch_size, self.num_units)
